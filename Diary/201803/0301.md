# 2018.3.1  
## 学习内容  
* Tensorflow  
循环神经网络，长短时记忆网络，深层循环神经网络，dropout，自然语言建模，时间序列预测，PTB文本数据，tensorboard，GPU计算加速  
## 学习进度  
### Tensorflow  
* 循环神经网络  
常用于自然语言处理，时刻作为一个重要概念引入，当前神经网络的状态又上一个时刻的状态和输入共同决定，可视为同一个神经网络结构在时间序列上
被复制多次的结果。循环神经网络的总损失是所有时刻的损失函数之和。  
* 长短时记忆网络（LSTM）：靠“门”的结构，让信息有选择性地影响循环神经网络每个时刻的状态。LSTM有三个门，遗忘门，输入门，输出门。遗忘门会根据当前的输入，
上一时刻的状态和上一时刻的输出共同决定哪一部分记忆需要被遗忘。忘记之后，还需要从当前的输入补充最新的记忆，这个部分由输入门完成。具体实现较为复杂，tensorflow
封装简单，参考如下：  
```
lstm = rnn_cell_BascLSTMCell(lstm_hidden_size)
state = lstm.zero_state(batch_size, tf.float32)
loss = 0.0
for i in range(num_steps):
    if i > 0: tf.get_variable_scope().reuse_variables()
    lstm_out_put, state = lstm(current_input, state)
    final_output = fully_connected(lstm_output)
    loss += calc_loss(final_output, expected_output) 
```
* 深层循环神经网络  
在深层神经网络上在封装一层MultiRNNCell即可，参考代码如下：  
```
stm = rnn_cell_BascLSTMCell(lstm_hidden_size)
stacked_lstm = rnn_cell.MultiRNNCell([lstm] * number_of_layers)
state = stacked_lstm.zero_state(batch_size, tf.float32)
loss = 0.0
for i in range(num_steps):
    if i > 0: tf.get_variable_scope().reuse_variables()
    stacked_lstm_out_put, state = stacked_lstm(current_input, state)
    final_output = fully_connected(stacked_lstm_output)
    loss += calc_loss(final_output, expected_output)  
```
* dropout  
类似卷及神经网络中的应用，可使神经网络更加健壮。卷积神经网络一般只在最后的全连接层dropout，循环神经网络一般只在层级之间dropout，不会发生在
时刻之间。实例代码如下：  
```
lstm = rnn_cell.BasicLSTMCell(lstm_size)
dropout_lstm = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob = 0.5)
stacked_state = rnn_cell.MultiRNNCell([dropout_lstm] * number_of_layers)
```
* 自然语言建模：一个句子可视为一个单词序列，S=(w1,w2,w3,...,wm)的概率可表示为p(S)=p(w1,w2,w3,...,wm)=p(w1)p(w2|w1)p(w3|w1,w2)...  
要知道总概率，就需要知道每一项的概率。常见的估计方法有n-gram，决策树，最大熵模型，条件随机场，神经网络语言模型等。n-gram模型有一个有限历史假设：
当前词出现的概率仅与前n-1个单词有关。语言模型的好坏常用的评价指标是复杂度（preplexity），该值刻画的是通过某一模型估计的下一句话出现的概率。若一个
模型的复杂度为n，则表示，模型预测下一个词时，有89个词等可能地可以作为下一个词的合理选择。循环神经网络也可对语言进行建模。  
* PTB文本数据  
ptb_iterator函数，对数据进行截断和组织成batch。batch大小为纵向数量n，截断长度为横向长度m，结合起来就是一个batch块，数组大小为[n,m]。  
* 时间序列预测  
主要提到TFLearn可以简化代码和训练模型，类似slim，封装了一些常用的网络结构，直接使用，提高代码编写效率。  
* TensorBoard  
运行tensorboard：tensorboard --logdir=/path/to/log  
通过浏览器打开localhost:6006即可访问界面。  
运用命名空间可以简化计算图，优化可视化结果。  
监控指标，节点信息均可通过加入相应代码显示在界面上。  
* GPU计算加速  
GPU使用可指定device，有些操作无法运行于GPU，支持分布式（同步，异步）执行。  
## 学习建议  
补充学习词向量概念，tensorboard部分和循环网络部分需要补充代码实践。  
## END

