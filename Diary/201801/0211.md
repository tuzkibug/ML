# 2018.2.11  
## 学习内容  
* Tensorflow  
卷积神经网络相关  
## 学习进度  
* Tensorflow：基本概念，基本表达式，经典卷积神经网络的介绍和简单实现  
* 卷积层：处理后矩阵的深度增加。  
卷积层代码实现：  
  filter_weights = tf.get_variable('weights', [5, 5, 3, 16], initializer=tf.truncated_normal_initializer(stddev=0.1))  
  biases = tf.get_variable('biases', [16], initializer=tf.constant_initializer(0.1))  
  conv = tf.nn.conv2d(input, filter_weights, strides=[1, 1, 1, 1], padding='SAME')  
  bias = tf.nn.bias_add(conv, biases)  
  actived_conv = tf.nn.relu(bias)  
代码解释：  
filter_weights：过滤器，或者叫卷积核。5\*5代表长宽，3是输入层矩阵深度，16是卷积核深度，两者不一样。  
biases：16为卷积核深度  
conv：需要有输入矩阵，卷积核，步长，填充等参数。strides步长第一位最后一位必须为1，意思是不跨样例，不跨深度中间的1\*1为长宽方向的步长。padding为填充模式，
SAME为全零填充，VALID为不填充。全零填充后，结果长宽方向与原矩阵一致，不填充则缩小。  
bias：tf.nn.bias_add函数自动为所有节点加上配置项。  

* 池化层  
池化层代码实现：  
  pool = tf.nn.max_pool(actived_conv, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')  
ksize：池化层过滤器大小，第一位第四位必须为1，中间为长宽大小  
strides：同卷积层  
池化常用最大池化和平均池化tf.nn.max_pool和tf.nn.avg_pool  

* LeNet-5模型简介  
经典神经网络模型：输入层-->(卷积层+-->池化层?)+-->全连接层+  
LeNet-5模型：输入层-->卷积层-->池化层-->卷积层-->池化层-->全连接层-->全连接层-->输出层  

* Inception-v3模型简介  
使用不同大小的卷积核进行处理，由于使用了全零填充，不同卷积核处理后的结果长宽大小一致，拼接后形成新的处理结果，深度相加。  

## END
