# 2018.2.7  
## 学习内容  
* Tensorflow  
深层神经网络：线性模型，激活函数的去线性化，多层网络解决异或问题，损失函数，优化算法，学习率，过拟合问题，滑动平均模型
## 学习小结  
* 线性模型：基本掌握一般线性模型格式a=tf.matmul(x,w1)+biases1,y=tf.matmul(a,w2)+biases2，了解其具有局限性  
* 激活函数：为了将结果去线性化，加入激活函数概念，a=tf.nn.relu(tf.matmul(x,w1)+biases1),y=tf.nn.relu(tf.matmul(a,w2)+biases2)，tensorflow提供7种不同的激活函数，常用的激活函数有tf.nn.relu,tf.sigmoid,tf.tanh，tensorflow提供7种不同的激活函数  
* 损失函数：可使用交叉商，或者自定义。交叉熵一般与softmax回归同时使用，tensorflow做了打包封装，cross_entropy=tf.nn.softmax_cross_entropy_with_logits(y,y_)，y为计算值，y_为标准答案。自定义损失函数可记为loss=f(y,y_)。
* 优化算法：梯度优化等，梯度优化得到的是局部最优解，随机梯度优化快但是不保证取的局部最优解，结合两者的特点可用batch来定义每次优化的数据块。一般的优化算法有tf.train.AdamOptimizer,tf.train.GradientDescent.Optimizer,tf.train.MomentumOptimizer,一般使用时记为train_step=tf.train.AdamOptimizer(learning_rate).minimize(loss)。  
* 学习率：过大会导致不收敛，过小会导致收敛太慢，所以引入指数衰减学习率。tf.train.exponential_decay()。样例：learning_rate=tf.train.exponential_decay(0.1,global_step,100,0.96,staircase=True)。  
* 过拟合问题：为避免过拟合，引入正则化概念，使用时在损失函数中加入刻画模型复杂度的指标。记损失函数为J(o)，优化时，优化J(o)+λR(w)，R(w)刻画复杂程度，λ表示模型复杂损失在总损失中的比例。o包括w,b，但是一般复杂度只跟w有关。分为L1正则化，L2正则化。一般用法：loss=loss1+tf.contri.layers.l2_regularizer(lamda)(w)  
* 滑动平均模型：使模型在测试数据上更健壮(robust)的方法。tf.train.ExponentialMovingAverage。初始化该方法时，需要提供一个衰减率decay，每一个变量会维护一个影子变量shadow_variable，每次运行更新变量时，影子变量会更新为shadow_variable=decay\*shadow_variable+(1-decay)\*variable。实际应用中，decay会设置为接近1的数，如0.9999。为了前期训练更快，会提供一个num_updates参数，使decay={decay,(1+num_updates)/(1-num_updates)}，动态调整decay的大小。  


## END
